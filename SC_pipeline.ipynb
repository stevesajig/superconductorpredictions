{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Superconducting Materials Design with VAE + Random Forest + Nearest-Neighbor Mapping\n",
        "\n",
        "- Trains a RandomForestRegressor to predict critical temperature (Tc) from features.\n",
        "- Trains a VAE on the same feature space to learn a latent distribution.\n",
        "- Generates new candidate feature vectors using the VAE.\n",
        "- Predicts Tc of generated candidates with the Random Forest.\n",
        "- Maps generated candidates to the closest known compositions in unique_m.csv.\n",
        "\n"
      ],
      "metadata": {
        "id": "jYkJIP8EjrmH"
      },
      "id": "jYkJIP8EjrmH"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "-Yb3yKL8fk94"
      },
      "id": "-Yb3yKL8fk94",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "2cTXb2g5fk6R"
      },
      "id": "2cTXb2g5fk6R",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "TRAIN_CSV = \"train.csv\"\n",
        "UNIQUE_M_CSV = \"unique_m.csv\"\n",
        "OUT_GENERATED_CSV = \"generated_candidates_with_matches.csv\""
      ],
      "metadata": {
        "id": "1l6guJHdfk3f"
      },
      "id": "1l6guJHdfk3f",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Load and preprocess data\n",
        "# -----------------------------\n",
        "train_df = pd.read_csv(TRAIN_CSV)\n",
        "features = train_df.drop(columns=[\"critical_temp\"])\n",
        "target = train_df[\"critical_temp\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_scaled = scaler.fit_transform(features)\n",
        "\n",
        "# -----------------------------\n",
        "# Train / validation split\n",
        "# -----------------------------\n",
        "X_train, X_val, y_train, y_val = train_test_split(features_scaled, target, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "uYyNW0LGfk0v"
      },
      "id": "uYyNW0LGfk0v",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Random Forest regressor (Tc prediction)\n",
        "# -----------------------------\n",
        "regressor = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    n_jobs=-1)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_val_pred = regressor.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_val_pred)\n",
        "rmse = mse ** 0.5\n",
        "r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "print(f\"[RandomForest] Validation MSE : {mse:.3f}\")\n",
        "print(f\"[RandomForest] Validation RMSE: {rmse:.3f}\")\n",
        "print(f\"[RandomForest] Validation R^2 : {r2:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ul_3Nw4fkoT",
        "outputId": "f6f28bb9-5654-4022-e385-662602307779"
      },
      "id": "_Ul_3Nw4fkoT",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[RandomForest] Validation MSE : 80.643\n",
            "[RandomForest] Validation RMSE: 8.980\n",
            "[RandomForest] Validation R^2 : 0.930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# VAE model (unconditional VAE)\n",
        "# -----------------------------\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim=10):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.mu = nn.Linear(32, latent_dim)\n",
        "        self.log_var = nn.Linear(32, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, input_dim),\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        mu = self.mu(encoded)\n",
        "        log_var = self.log_var(encoded)\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        decoded = self.decoder(z)\n",
        "        return decoded, mu, log_var\n",
        "\n",
        "\n",
        "def vae_loss(recon_x, x, mu, log_var, beta=0.1):\n",
        "    # Reconstruction loss (MSE)\n",
        "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction=\"mean\")\n",
        "    # Standard KL divergence term\n",
        "    kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
        "\n",
        "\n",
        "input_dim = features_scaled.shape[1]\n",
        "latent_dim = 10\n",
        "vae = VAE(input_dim, latent_dim)"
      ],
      "metadata": {
        "id": "fpb_ouv6kL3r"
      },
      "id": "fpb_ouv6kL3r",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# VAE training setup\n",
        "# -----------------------------\n",
        "tensor_data = torch.tensor(features_scaled, dtype=torch.float32)\n",
        "dataset = TensorDataset(tensor_data)\n",
        "data_loader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
        "\n",
        "optimizer = optim.Adam(vae.parameters(), lr=1e-3)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    vae.train()\n",
        "    total_loss = 0.0\n",
        "    total_recon = 0.0\n",
        "    total_kl = 0.0\n",
        "\n",
        "    for (batch_x,) in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        recon_x, mu, log_var = vae(batch_x)\n",
        "        loss, recon_loss, kl_loss = vae_loss(recon_x, batch_x, mu, log_var, beta=0.1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_recon += recon_loss.item()\n",
        "        total_kl += kl_loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_recon = total_recon / len(data_loader)\n",
        "    avg_kl = total_kl / len(data_loader)\n",
        "\n",
        "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
        "        print(\n",
        "            f\"[VAE] Epoch {epoch+1}/{num_epochs} \"\n",
        "            f\"Loss: {avg_loss:.4f}, Recon: {avg_recon:.4f}, KL: {avg_kl:.4f}\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vc_FdaRkQHu",
        "outputId": "ad9475bf-d692-4617-c826-d2fa3a5b0264"
      },
      "id": "5vc_FdaRkQHu",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[VAE] Epoch 1/100 Loss: 0.8022, Recon: 0.7787, KL: 0.2352\n",
            "[VAE] Epoch 11/100 Loss: 0.2657, Recon: 0.1849, KL: 0.8078\n",
            "[VAE] Epoch 21/100 Loss: 0.2162, Recon: 0.1338, KL: 0.8237\n",
            "[VAE] Epoch 31/100 Loss: 0.2043, Recon: 0.1223, KL: 0.8205\n",
            "[VAE] Epoch 41/100 Loss: 0.1961, Recon: 0.1143, KL: 0.8176\n",
            "[VAE] Epoch 51/100 Loss: 0.1941, Recon: 0.1114, KL: 0.8275\n",
            "[VAE] Epoch 61/100 Loss: 0.1891, Recon: 0.1071, KL: 0.8205\n",
            "[VAE] Epoch 71/100 Loss: 0.1871, Recon: 0.1052, KL: 0.8183\n",
            "[VAE] Epoch 81/100 Loss: 0.1871, Recon: 0.1050, KL: 0.8207\n",
            "[VAE] Epoch 91/100 Loss: 0.1844, Recon: 0.1025, KL: 0.8186\n",
            "[VAE] Epoch 100/100 Loss: 0.1841, Recon: 0.1019, KL: 0.8218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Generate new candidates\n",
        "# -----------------------------\n",
        "def generate_new_materials(n_samples=50, z_scale=1.0):\n",
        "    \"\"\"\n",
        "    Sample from the VAE latent space and return:\n",
        "      - generated feature vectors in original scaling\n",
        "      - predicted Tc from the RandomForest regressor\n",
        "    \"\"\"\n",
        "    vae.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n_samples, latent_dim) * z_scale\n",
        "        generated_scaled = vae.decoder(z).detach().cpu().numpy()\n",
        "    # Inverse transform to original feature scale\n",
        "    generated_features = scaler.inverse_transform(generated_scaled)\n",
        "    generated_df = pd.DataFrame(generated_features, columns=features.columns)\n",
        "    generated_df[\"Predicted_Tc\"] = regressor.predict(generated_scaled)\n",
        "    return generated_df\n",
        "\n",
        "\n",
        "n_samples = 100  # adjust as needed\n",
        "generated_df = generate_new_materials(n_samples=n_samples, z_scale=1.0)\n",
        "\n",
        "# Optionally filter candidates by high predicted Tc\n",
        "HIGH_TC_THRESHOLD = 80.0  # K, for example\n",
        "high_tc_df = generated_df[generated_df[\"Predicted_Tc\"] >= HIGH_TC_THRESHOLD].copy()\n",
        "high_tc_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"Generated {len(generated_df)} candidates.\")\n",
        "print(f\"{len(high_tc_df)} candidates with Predicted_Tc >= {HIGH_TC_THRESHOLD} K.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuQ74M3pkWDs",
        "outputId": "1faae971-b093-4901-95dd-1a2dad4248de"
      },
      "id": "tuQ74M3pkWDs",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 100 candidates.\n",
            "2 candidates with Predicted_Tc >= 80.0 K.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Map to closest known compositions (unique_m)\n",
        "# -----------------------------\n",
        "# The original issue is that `unique_m_df` (from unique_m.csv) contains elemental compositions\n",
        "# (e.g., 'H', 'He', 'Li'), while `high_tc_df` (generated candidates) contains\n",
        "# descriptive material properties (e.g., 'number_of_elements', 'mean_atomic_mass').\n",
        "# This mismatch meant there were no common feature columns for comparison, leading to ValueError.\n",
        "\n",
        "# To fix this, we need to ensure that the dataset used for `unique_features` has\n",
        "# the same descriptive feature columns as `high_tc_df`.\n",
        "# The `train_df` already contains these descriptive features along with `critical_temp`.\n",
        "# `unique_m_df` (from `unique_m.csv`) contains `material` identifiers and `critical_temp`.\n",
        "\n",
        "# We will use the descriptive features from `train_df` as our reference for \"known compositions\"\n",
        "# for the purpose of distance calculation. We will then retrieve the 'material' and\n",
        "# 'critical_temp' from `unique_m_df` for the closest matches, leveraging the implicit\n",
        "# row-wise correspondence between `train_df` and `unique_m_df` (both have 21263 rows).\n",
        "\n",
        "# Load unique_m.csv to get material names and known critical temperatures.\n",
        "# We will use its `material` and `critical_temp` columns later.\n",
        "unique_m_info_df = pd.read_csv(UNIQUE_M_CSV)\n",
        "unique_m_info_df.columns = unique_m_info_df.columns.str.strip()\n",
        "\n",
        "# Prepare the reference DataFrame for matching, using descriptive features from train_df.\n",
        "# `train_df` contains the same descriptive features as `high_tc_df` (excluding 'Predicted_Tc').\n",
        "# We define the feature columns to use for matching based on the original `features` DataFrame.\n",
        "feature_columns_for_match = features.columns.tolist() # These are the descriptive feature names\n",
        "\n",
        "# The 'unique_features' for distance calculation will be the descriptive features from `train_df`.\n",
        "unique_features = train_df[feature_columns_for_match].values\n",
        "\n",
        "# The generated candidates from high_tc_df also need to be restricted to these same feature columns\n",
        "# for consistent comparison.\n",
        "generated_features_for_match = high_tc_df[feature_columns_for_match].values\n",
        "\n",
        "# Compute distances and get nearest neighbors\n",
        "distances = cdist(generated_features_for_match, unique_features, metric=\"euclidean\")\n",
        "closest_indices = np.argmin(distances, axis=1)\n",
        "\n",
        "# Retrieve the 'material' and 'critical_temp' for the closest matches.\n",
        "# We use `unique_m_info_df` for this, assuming its rows correspond to `train_df`'s rows\n",
        "# based on the matching `closest_indices`.\n",
        "closest_materials = unique_m_info_df.iloc[closest_indices].reset_index(drop=True)\n",
        "\n",
        "# Attach matched material and its known Tc\n",
        "high_tc_df[\"Closest_Material\"] = closest_materials[\"material\"].values\n",
        "high_tc_df[\"Known_Tc_Closest\"] = closest_materials[\"critical_temp\"].values\n",
        "\n",
        "# Tc consistency metric\n",
        "high_tc_df[\"Tc_Diff\"] = high_tc_df[\"Predicted_Tc\"] - high_tc_df[\"Known_Tc_Closest\"]\n"
      ],
      "metadata": {
        "id": "8X4qaOTzkaW7"
      },
      "id": "8X4qaOTzkaW7",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Save results\n",
        "# -----------------------------\n",
        "high_tc_df.to_csv(OUT_GENERATED_CSV, index=False)\n",
        "print(f\"Saved high-Tc candidates with nearest known matches to {OUT_GENERATED_CSV}\")\n",
        "\n",
        "# Preview\n",
        "print(high_tc_df[[\"Predicted_Tc\", \"Closest_Material\", \"Known_Tc_Closest\", \"Tc_Diff\"]]\n",
        "      .sort_values(by=\"Predicted_Tc\", ascending=False)\n",
        "      .head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyNQ4ZPsfkkw",
        "outputId": "8aa79756-ffc7-4dee-e7fc-6c24b2aace23"
      },
      "id": "pyNQ4ZPsfkkw",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved high-Tc candidates with nearest known matches to generated_candidates_with_matches.csv\n",
            "   Predicted_Tc          Closest_Material  Known_Tc_Closest    Tc_Diff\n",
            "0    100.729026             Hg1Ba2Ca1Cu2O             125.0 -24.270974\n",
            "1     80.406248  Pb0.7Sr2Ca0.5Y0.5Cu2.3O7              40.0  40.406248\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}