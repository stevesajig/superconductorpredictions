{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb75791a-fd7a-4400-b25a-d30ffd2600c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 1.1599, MSE: 1.1478, KL: 0.1201\n",
      "Epoch 10/100, Loss: 0.9096, MSE: 0.9015, KL: 0.0807\n",
      "Epoch 20/100, Loss: 0.8445, MSE: 0.8357, KL: 0.0881\n",
      "Epoch 30/100, Loss: 0.8171, MSE: 0.8071, KL: 0.1007\n",
      "Epoch 40/100, Loss: 0.8035, MSE: 0.7926, KL: 0.1082\n",
      "Epoch 50/100, Loss: 0.7975, MSE: 0.7862, KL: 0.1123\n",
      "Epoch 60/100, Loss: 0.7936, MSE: 0.7821, KL: 0.1144\n",
      "Epoch 70/100, Loss: 0.7923, MSE: 0.7808, KL: 0.1155\n",
      "Epoch 80/100, Loss: 0.7917, MSE: 0.7801, KL: 0.1161\n",
      "Epoch 90/100, Loss: 0.7913, MSE: 0.7796, KL: 0.1163\n",
      "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
      "0           -0.614181          0.480965              0.322492   \n",
      "1           -0.298861         -0.075273             -0.348898   \n",
      "2            0.249359         -0.165452             -0.079507   \n",
      "\n",
      "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
      "0           0.410336               0.259594            -0.722654   \n",
      "1          -0.401048              -0.129453             0.061744   \n",
      "2           0.039183              -0.108753             0.401055   \n",
      "\n",
      "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
      "0                -0.811865          -0.201403               0.266093   \n",
      "1                 0.104493          -0.168323              -0.147387   \n",
      "2                 0.541673           0.619672              -0.256555   \n",
      "\n",
      "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
      "0        -0.453250  ...          0.432430       0.228692           0.590728   \n",
      "1         0.282126  ...         -0.000824      -0.218810          -0.091627   \n",
      "2         0.376140  ...         -0.619260      -0.583402          -0.387554   \n",
      "\n",
      "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
      "0        -0.903602            -1.007627      -0.119849          -0.124054   \n",
      "1         0.446211             0.431625       0.323052          -0.031839   \n",
      "2         0.471865             0.561547      -0.088915          -0.430869   \n",
      "\n",
      "   std_Valence  wtd_std_Valence  Predicted_Tc  \n",
      "0     0.212842         0.257280         100.0  \n",
      "1     0.013130         0.043065         120.0  \n",
      "2    -0.020044        -0.299552         150.0  \n",
      "\n",
      "[3 rows x 82 columns]\n",
      "  Closest_Material  predicted_tc\n",
      "0  Ba0.2La1.8Cu1O4         100.0\n",
      "1  Ba0.2La1.8Cu1O4         120.0\n",
      "2  Ba0.2La1.8Cu1O4         150.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\train.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=[\"critical_temp\"])\n",
    "target = df[\"critical_temp\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "Tc_tensor = torch.tensor(target.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "input_dim = X_tensor.shape[1]\n",
    "condition_dim = 1  # Temperature as condition\n",
    "latent_dim = 10  # Adjust if necessary\n",
    "\n",
    "class OptimizedCVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim=10):\n",
    "        super(OptimizedCVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim * 2)  # Mean and log-variance\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, condition):\n",
    "        x = torch.cat([x, condition], dim=1)\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, condition):\n",
    "        x = torch.cat([z, condition], dim=1)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x, condition):\n",
    "        mu, log_var = self.encode(x, condition)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed_x = self.decode(z, condition)\n",
    "        return reconstructed_x, mu, log_var\n",
    "\n",
    "def cvae_loss(recon_x, x, mu, log_var, beta=0.1):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - (log_var.exp()))\n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Initialize CVAE\n",
    "cvae = OptimizedCVAE(input_dim, condition_dim, latent_dim)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    recon_x, mu, log_var = cvae(X_tensor, Tc_tensor)\n",
    "    loss, mse, kl = cvae_loss(recon_x, X_tensor, mu, log_var)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}, MSE: {mse.item():.4f}, KL: {kl.item():.4f}\")\n",
    "\n",
    "# Generate new materials\n",
    "high_tc_values = torch.tensor([[100.0], [120.0], [150.0]], dtype=torch.float32)\n",
    "z = torch.randn((3, latent_dim))\n",
    "generated_materials = cvae.decode(z, high_tc_values).detach().numpy()\n",
    "\n",
    "# Save generated materials\n",
    "generated_df = pd.DataFrame(generated_materials, columns=features.columns)\n",
    "generated_df[\"Predicted_Tc\"] = high_tc_values.numpy()\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\optimized_materials.csv\", index=False)\n",
    "\n",
    "print(generated_df.head())\n",
    "\n",
    "\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\", index=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "### ðŸ”¹ Step 1: Load and Preprocess Datasets\n",
    "\n",
    "# Load known superconductors dataset\n",
    "unique_m_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\unique_m.csv\")  # Replace with actual file name\n",
    "\n",
    "# Load generated superconducting materials with predicted Tc\n",
    "generated_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\")  # Replace with actual file name\n",
    "\n",
    "# Standardize column names\n",
    "unique_m_df.columns = unique_m_df.columns.str.strip().str.lower()\n",
    "generated_df.columns = generated_df.columns.str.strip().str.lower()\n",
    "\n",
    "### ðŸ”¹ Step 2: Ensure Predicted Tc Column Exists\n",
    "predicted_tc_col = \"Predicted_tc\" if \"predicted_tc\" in generated_df.columns else None\n",
    "\n",
    "if predicted_tc_col is None:\n",
    "    raise KeyError(\"Column 'Predicted_Tc' not found in generated_high_tc_df\")\n",
    "predicted_tc_values = generated_df[\"predicted_tc\"]\n",
    "### ðŸ”¹ Step 3: Identify Common Feature Columns\n",
    "feature_columns = [col for col in generated_df.columns if col in unique_m_df.columns and col != predicted_tc_col]\n",
    "\n",
    "# Warn if features are missing in unique_m_df\n",
    "missing_cols = [col for col in feature_columns if col not in unique_m_df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: The following columns are missing in unique_m_df and will be ignored:\", missing_cols)\n",
    "\n",
    "### ðŸ”¹ Step 4: Compute Similarity Between Generated and Known Materials\n",
    "\n",
    "# Select only common features\n",
    "unique_features = unique_m_df[feature_columns]\n",
    "generated_features = generated_df[feature_columns]\n",
    "\n",
    "# Calculate Euclidean distance between generated materials and known materials\n",
    "distances = cdist(generated_features, unique_features, metric='euclidean')\n",
    "\n",
    "# Find closest known material for each generated one\n",
    "closest_indices = np.argmin(distances, axis=1)\n",
    "closest_materials = unique_m_df.iloc[closest_indices].reset_index(drop=True)\n",
    "\n",
    "### ðŸ”¹ Step 5: Append Closest Known Material Names\n",
    "\n",
    "# Ensure the column name for materials exists\n",
    "if \"material\" in unique_m_df.columns:\n",
    "    generated_df[\"Closest_Material\"] = closest_materials[\"material\"]\n",
    "else:\n",
    "    raise KeyError(\"Column 'material_name' not found in unique_m_df. Check the dataset format.\")\n",
    "\n",
    "### ðŸ”¹ Step 6: Save and Display Final Results\n",
    "\n",
    "# Save the final dataset with matched materials\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\", index=False)\n",
    "\n",
    "# Print a preview of matched materials\n",
    "print(generated_df[[\"Closest_Material\", \"predicted_tc\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfc80da-dcf5-4335-9f9a-6e0c7b63eb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Superconducting Materials:\n",
      "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
      "0            4.314554         96.704819            101.697472   \n",
      "1            4.028898         93.952789             95.707382   \n",
      "2            3.530948         91.329567             75.883621   \n",
      "3            3.944969         97.449356            112.907776   \n",
      "4            4.062790         74.173264             64.599052   \n",
      "\n",
      "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
      "0          86.325706              88.691605             1.377314   \n",
      "1          81.148354              86.459747             1.268411   \n",
      "2          74.643791              61.538116             1.036852   \n",
      "3          88.891891             102.911377             1.350034   \n",
      "4          68.475395              56.998871             1.402480   \n",
      "\n",
      "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
      "0                 1.226459         130.439835              45.736702   \n",
      "1                 1.166801         116.039368              32.624187   \n",
      "2                 1.102904         120.681671              23.919575   \n",
      "3                 0.927191          98.552002              69.198967   \n",
      "4                 1.243239          66.041199              18.336069   \n",
      "\n",
      "   std_atomic_mass  ...  mean_Valence  wtd_mean_Valence  gmean_Valence  \\\n",
      "0        49.996307  ...      3.178163          3.253054       3.025889   \n",
      "1        46.829205  ...      2.963911          3.043889       2.771930   \n",
      "2        50.503399  ...      3.395379          3.086257       3.292866   \n",
      "3        42.035160  ...      2.277614          2.049688       2.065988   \n",
      "4        25.175180  ...      2.479269          2.278319       2.424588   \n",
      "\n",
      "   wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n",
      "0           3.069949         1.432871             1.319029       2.575382   \n",
      "1           2.904176         1.341378             1.190420       2.661899   \n",
      "2           2.978999         1.203769             1.133938       2.027544   \n",
      "3           1.871936         1.354071             1.098403       2.675841   \n",
      "4           2.286902         1.393372             1.179613       1.412778   \n",
      "\n",
      "   wtd_range_Valence  std_Valence  wtd_std_Valence  \n",
      "0           0.999101     1.025996         1.007573  \n",
      "1           1.113082     1.155649         0.892847  \n",
      "2           0.784836     0.891137         0.819059  \n",
      "3           1.058109     1.174677         1.034193  \n",
      "4           0.934683     0.563966         0.236474  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "# Generate new superconducting materials\n",
    "def generate_new_materials(n_samples=10):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim)\n",
    "        generated_data = vae.decoder(z).numpy()\n",
    "        generated_data = scaler.inverse_transform(generated_data)  # Convert back to original scale\n",
    "        return pd.DataFrame(generated_data, columns=features.columns)\n",
    "\n",
    "# Generate and display new material candidates\n",
    "new_materials = generate_new_materials(10)\n",
    "print(\"Generated Superconducting Materials:\")\n",
    "print(new_materials.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9729066-f9d5-4644-8366-649dc1489502",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Train a regression model to predict critical temperature\u001b[39;00m\n\u001b[0;32m     19\u001b[0m regressor \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m regressor\u001b[38;5;241m.\u001b[39mfit(features_scaled, target)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Define the VAE model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mVAE\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    492\u001b[0m     prefer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreads\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    493\u001b[0m )(\n\u001b[0;32m    494\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    495\u001b[0m         t,\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap,\n\u001b[0;32m    497\u001b[0m         X,\n\u001b[0;32m    498\u001b[0m         y,\n\u001b[0;32m    499\u001b[0m         sample_weight,\n\u001b[0;32m    500\u001b[0m         i,\n\u001b[0;32m    501\u001b[0m         \u001b[38;5;28mlen\u001b[39m(trees),\n\u001b[0;32m    502\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    503\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight,\n\u001b[0;32m    504\u001b[0m         n_samples_bootstrap\u001b[38;5;241m=\u001b[39mn_samples_bootstrap,\n\u001b[0;32m    505\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    506\u001b[0m     )\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    193\u001b[0m         X,\n\u001b[0;32m    194\u001b[0m         y,\n\u001b[0;32m    195\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39mcurr_sample_weight,\n\u001b[0;32m    196\u001b[0m         check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    197\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m builder\u001b[38;5;241m.\u001b[39mbuild(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load and preprocess dataset\n",
    "train_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\train.csv\")\n",
    "features = train_df.drop(columns=[\"critical_temp\"])\n",
    "target = train_df[\"critical_temp\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Train a regression model to predict critical temperature\n",
    "regressor = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "regressor.fit(features_scaled, target)\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=10):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.mu = nn.Linear(32, latent_dim)\n",
    "        self.log_var = nn.Linear(32, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mu, log_var = self.mu(encoded), self.log_var(encoded)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, log_var\n",
    "\n",
    "# Convert to tensor and create DataLoader\n",
    "tensor_data = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "data_loader = DataLoader(TensorDataset(tensor_data), batch_size=64, shuffle=True)\n",
    "\n",
    "# Define training parameters\n",
    "input_dim = features_scaled.shape[1]\n",
    "latent_dim = 10\n",
    "epochs = 100\n",
    "vae = VAE(input_dim, latent_dim)\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "def vae_loss(recon_x, x, mu, log_var):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return recon_loss + 0.1 * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        x = batch[0]\n",
    "        optimizer.zero_grad()\n",
    "        decoded, mu, log_var = vae(x)\n",
    "        loss, recon_loss, kl_loss = vae_loss(decoded, x, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(data_loader):.4f}\")\n",
    "\n",
    "# Generate new superconducting materials\n",
    "def generate_new_materials(n_samples=10):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim)\n",
    "        generated_data = vae.decoder(z).numpy()\n",
    "        generated_data = scaler.inverse_transform(generated_data)\n",
    "        generated_df = pd.DataFrame(generated_data, columns=features.columns)\n",
    "        generated_df[\"Predicted_Tc\"] = regressor.predict(generated_data)\n",
    "        return generated_df\n",
    "\n",
    "# Generate and display new material candidates\n",
    "new_materials = generate_new_materials(10)\n",
    "print(new_materials.sort_values(by=\"Predicted_Tc\", ascending=False).head())\n",
    "\n",
    "# Save generated materials\n",
    "new_materials.to_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials_optimized.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9d7db739-e109-4fd2-b9a7-e3e97d2901f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 1.2349, MSE: 1.2053, KL: 0.2967\n",
      "Epoch 10/100, Loss: 0.9410, MSE: 0.9317, KL: 0.0931\n",
      "Epoch 20/100, Loss: 0.8692, MSE: 0.8632, KL: 0.0594\n",
      "Epoch 30/100, Loss: 0.8408, MSE: 0.8357, KL: 0.0511\n",
      "Epoch 40/100, Loss: 0.8283, MSE: 0.8233, KL: 0.0498\n",
      "Epoch 50/100, Loss: 0.8218, MSE: 0.8168, KL: 0.0501\n",
      "Epoch 60/100, Loss: 0.8187, MSE: 0.8136, KL: 0.0506\n",
      "Epoch 70/100, Loss: 0.8168, MSE: 0.8117, KL: 0.0509\n",
      "Epoch 80/100, Loss: 0.8163, MSE: 0.8112, KL: 0.0511\n",
      "Epoch 90/100, Loss: 0.8154, MSE: 0.8103, KL: 0.0511\n",
      "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
      "0           -0.644404          0.345952              0.174679   \n",
      "1            0.011302         -0.263981             -0.382976   \n",
      "2            0.486185         -0.093396             -0.101891   \n",
      "\n",
      "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
      "0           0.686918               0.337740            -0.576999   \n",
      "1          -0.375970              -0.144822             0.138584   \n",
      "2          -0.211729              -0.399633             0.542846   \n",
      "\n",
      "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
      "0                -0.652392          -0.709953               0.360105   \n",
      "1                 0.281157           0.000072              -0.305083   \n",
      "2                 0.827253           0.619689              -0.337631   \n",
      "\n",
      "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
      "0        -0.556230  ...          0.998295       0.085298           0.395741   \n",
      "1         0.155116  ...         -0.086005      -0.041651          -0.459441   \n",
      "2         0.497268  ...         -0.469507      -0.614589          -0.295951   \n",
      "\n",
      "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
      "0        -0.607575            -0.602451       0.010874           0.698516   \n",
      "1         0.174927            -0.331788       0.017656          -0.127616   \n",
      "2         0.255233             0.700118      -0.178818          -0.519693   \n",
      "\n",
      "   std_Valence  wtd_std_Valence  Predicted_Tc  \n",
      "0     0.412738         0.200981         100.0  \n",
      "1    -0.126341        -0.249596         120.0  \n",
      "2    -0.319629        -0.164986         150.0  \n",
      "\n",
      "[3 rows x 82 columns]\n",
      "  Closest_Material  predicted_tc\n",
      "0  Ba0.2La1.8Cu1O4         100.0\n",
      "1  Ba0.2La1.8Cu1O4         120.0\n",
      "2  Ba0.2La1.8Cu1O4         150.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 174\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(generated_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClosest_Material\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_tc\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m    173\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m--> 174\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot((loss, epochs), losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    175\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot((kl, epochs), kl_divergences, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL Divergence\u001b[39m\u001b[38;5;124m\"\u001b[39m, color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    176\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\pyplot.py:3590\u001b[0m, in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[0;32m   3583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[1;32m-> 3590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m gca()\u001b[38;5;241m.\u001b[39mplot(\n\u001b[0;32m   3591\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[0;32m   3592\u001b[0m         scalex\u001b[38;5;241m=\u001b[39mscalex,\n\u001b[0;32m   3593\u001b[0m         scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[0;32m   3594\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m   3595\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3596\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_axes.py:1724\u001b[0m, in \u001b[0;36mAxes.plot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1482\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[0;32m   1483\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[1;32m-> 1724\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m   1726\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[1;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_args(\n\u001b[0;32m    304\u001b[0m     axes, this, kwargs, ambiguous_fmt_datakey\u001b[38;5;241m=\u001b[39mambiguous_fmt_datakey)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\axes\\_base.py:488\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[1;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[0;32m    485\u001b[0m         kw[prop_name] \u001b[38;5;241m=\u001b[39m val\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(xy) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 488\u001b[0m     x \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    489\u001b[0m     y \u001b[38;5;241m=\u001b[39m _check_1d(xy[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\matplotlib\\cbook.py:1358\u001b[0m, in \u001b[0;36m_check_1d\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;66;03m# plot requires `shape` and `ndim`.  If passed an\u001b[39;00m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;66;03m# object that doesn't provide them, then force to numpy array.\u001b[39;00m\n\u001b[0;32m   1354\u001b[0m \u001b[38;5;66;03m# Note this will strip unit information.\u001b[39;00m\n\u001b[0;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1356\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m         \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m-> 1358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39matleast_1d(x)\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\shape_base.py:65\u001b[0m, in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     63\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ary \u001b[38;5;129;01min\u001b[39;00m arys:\n\u001b[1;32m---> 65\u001b[0m     ary \u001b[38;5;241m=\u001b[39m asanyarray(ary)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ary\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     67\u001b[0m         result \u001b[38;5;241m=\u001b[39m ary\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:1194\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\train.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=[\"critical_temp\"])\n",
    "target = df[\"critical_temp\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "Tc_tensor = torch.tensor(target.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "input_dim = X_tensor.shape[1]\n",
    "condition_dim = 1  # Temperature as condition\n",
    "latent_dim = 10  # Adjust if necessary\n",
    "\n",
    "class OptimizedCVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim=10):\n",
    "        super(OptimizedCVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim * 2)  # Mean and log-variance\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, condition):\n",
    "        x = torch.cat([x, condition], dim=1)\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, condition):\n",
    "        x = torch.cat([z, condition], dim=1)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x, condition):\n",
    "        mu, log_var = self.encode(x, condition)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed_x = self.decode(z, condition)\n",
    "        return reconstructed_x, mu, log_var\n",
    "\n",
    "def cvae_loss(recon_x, x, mu, log_var, beta=0.1):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - (log_var.exp()**2))\n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Initialize CVAE\n",
    "cvae = OptimizedCVAE(input_dim, condition_dim, latent_dim)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    recon_x, mu, log_var = cvae(X_tensor, Tc_tensor)\n",
    "    loss, mse, kl = cvae_loss(recon_x, X_tensor, mu, log_var)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item():.4f}, MSE: {mse.item():.4f}, KL: {kl.item():.4f}\")\n",
    "\n",
    "# Generate new materials\n",
    "high_tc_values = torch.tensor([[100.0], [120.0], [150.0]], dtype=torch.float32)\n",
    "z = torch.randn((3, latent_dim))\n",
    "generated_materials = cvae.decode(z, high_tc_values).detach().numpy()\n",
    "\n",
    "# Save generated materials\n",
    "generated_df = pd.DataFrame(generated_materials, columns=features.columns)\n",
    "generated_df[\"Predicted_Tc\"] = high_tc_values.numpy()\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\optimized_materials.csv\", index=False)\n",
    "\n",
    "print(generated_df.head())\n",
    "\n",
    "\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\", index=False)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "\n",
    "### ðŸ”¹ Step 1: Load and Preprocess Datasets\n",
    "\n",
    "# Load known superconductors dataset\n",
    "unique_m_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\unique_m.csv\")  # Replace with actual file name\n",
    "\n",
    "# Load generated superconducting materials with predicted Tc\n",
    "generated_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\")  # Replace with actual file name\n",
    "\n",
    "# Standardize column names\n",
    "unique_m_df.columns = unique_m_df.columns.str.strip().str.lower()\n",
    "generated_df.columns = generated_df.columns.str.strip().str.lower()\n",
    "\n",
    "### ðŸ”¹ Step 2: Ensure Predicted Tc Column Exists\n",
    "predicted_tc_col = \"Predicted_tc\" if \"predicted_tc\" in generated_df.columns else None\n",
    "\n",
    "if predicted_tc_col is None:\n",
    "    raise KeyError(\"Column 'Predicted_Tc' not found in generated_high_tc_df\")\n",
    "predicted_tc_values = generated_df[\"predicted_tc\"]\n",
    "### ðŸ”¹ Step 3: Identify Common Feature Columns\n",
    "feature_columns = [col for col in generated_df.columns if col in unique_m_df.columns and col != predicted_tc_col]\n",
    "\n",
    "# Warn if features are missing in unique_m_df\n",
    "missing_cols = [col for col in feature_columns if col not in unique_m_df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: The following columns are missing in unique_m_df and will be ignored:\", missing_cols)\n",
    "\n",
    "### ðŸ”¹ Step 4: Compute Similarity Between Generated and Known Materials\n",
    "\n",
    "# Select only common features\n",
    "unique_features = unique_m_df[feature_columns]\n",
    "generated_features = generated_df[feature_columns]\n",
    "\n",
    "# Calculate Euclidean distance between generated materials and known materials\n",
    "distances = cdist(generated_features, unique_features, metric='euclidean')\n",
    "\n",
    "# Find closest known material for each generated one\n",
    "closest_indices = np.argmin(distances, axis=1)\n",
    "closest_materials = unique_m_df.iloc[closest_indices].reset_index(drop=True)\n",
    "\n",
    "### ðŸ”¹ Step 5: Append Closest Known Material Names\n",
    "\n",
    "# Ensure the column name for materials exists\n",
    "if \"material\" in unique_m_df.columns:\n",
    "    generated_df[\"Closest_Material\"] = closest_materials[\"material\"]\n",
    "else:\n",
    "    raise KeyError(\"Column 'material_name' not found in unique_m_df. Check the dataset format.\")\n",
    "\n",
    "### ðŸ”¹ Step 6: Save and Display Final Results\n",
    "\n",
    "# Save the final dataset with matched materials\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\", index=False)\n",
    "\n",
    "# Print a preview of matched materials\n",
    "print(generated_df[[\"Closest_Material\", \"predicted_tc\"]].head())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot((loss, epochs), losses, label=\"Total Loss\", color='blue')\n",
    "plt.plot((kl, epochs), kl_divergences, label=\"KL Divergence\", color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss / KL Divergence\")\n",
    "plt.title(\"VAE Training: Loss and KL Divergence per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "967ab985-d10f-4076-8853-aea5eb96ae39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 0.4898, MSE: 0.2782, KL: 0.4065\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m loss, mse, kl \u001b[38;5;241m=\u001b[39m cvae_loss(recon_x, x, mu, log_var)\n\u001b[0;32m    107\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 108\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    110\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    111\u001b[0m total_kl \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m kl\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m opt \u001b[38;5;241m=\u001b[39m opt_ref()\n\u001b[0;32m    139\u001b[0m opt\u001b[38;5;241m.\u001b[39m_opt_called \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(opt, opt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    235\u001b[0m         group,\n\u001b[0;32m    236\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m         state_steps,\n\u001b[0;32m    242\u001b[0m     )\n\u001b[1;32m--> 244\u001b[0m     adam(\n\u001b[0;32m    245\u001b[0m         params_with_grad,\n\u001b[0;32m    246\u001b[0m         grads,\n\u001b[0;32m    247\u001b[0m         exp_avgs,\n\u001b[0;32m    248\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    249\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    250\u001b[0m         state_steps,\n\u001b[0;32m    251\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    252\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    253\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    254\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    255\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    256\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    257\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    258\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    259\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    260\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    261\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    262\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    263\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    264\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    265\u001b[0m     )\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 876\u001b[0m func(\n\u001b[0;32m    877\u001b[0m     params,\n\u001b[0;32m    878\u001b[0m     grads,\n\u001b[0;32m    879\u001b[0m     exp_avgs,\n\u001b[0;32m    880\u001b[0m     exp_avg_sqs,\n\u001b[0;32m    881\u001b[0m     max_exp_avg_sqs,\n\u001b[0;32m    882\u001b[0m     state_steps,\n\u001b[0;32m    883\u001b[0m     amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m    884\u001b[0m     has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    885\u001b[0m     beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[0;32m    886\u001b[0m     beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[0;32m    887\u001b[0m     lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[0;32m    888\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m    889\u001b[0m     eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m    890\u001b[0m     maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[0;32m    891\u001b[0m     capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m    892\u001b[0m     differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[0;32m    893\u001b[0m     grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[0;32m    894\u001b[0m     found_inf\u001b[38;5;241m=\u001b[39mfound_inf,\n\u001b[0;32m    895\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\optim\\adam.py:476\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    474\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 476\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    478\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\train.csv\")\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=[\"critical_temp\"])\n",
    "target = df[\"critical_temp\"]\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.tensor(features_scaled, dtype=torch.float32)\n",
    "Tc_tensor = torch.tensor(target.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(X_tensor, Tc_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_dim = X_tensor.shape[1]\n",
    "condition_dim = 1  # Temperature as condition\n",
    "latent_dim = 10  # Adjust if necessary\n",
    "\n",
    "class OptimizedCVAE(nn.Module):\n",
    "    def __init__(self, input_dim, condition_dim, latent_dim=10):\n",
    "        super(OptimizedCVAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim + condition_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, latent_dim * 2)  # Mean and log-variance\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + condition_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x, condition):\n",
    "        x = torch.cat([x, condition], dim=1)\n",
    "        h = self.encoder(x)\n",
    "        mu, log_var = torch.chunk(h, 2, dim=1)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, condition):\n",
    "        x = torch.cat([z, condition], dim=1)\n",
    "        return self.decoder(x)\n",
    "    \n",
    "    def forward(self, x, condition):\n",
    "        mu, log_var = self.encode(x, condition)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        reconstructed_x = self.decode(z, condition)\n",
    "        return reconstructed_x, mu, log_var\n",
    "\n",
    "def cvae_loss(recon_x, x, mu, log_var, beta=0.1):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.mean(1 + log_var - mu.pow(2) - (log_var.exp()**2))\n",
    "    return recon_loss + beta * kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Initialize CVAE\n",
    "cvae = OptimizedCVAE(input_dim, condition_dim, latent_dim)\n",
    "optimizer = optim.Adam(cvae.parameters(), lr=0.0005)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "# Lists to store loss values\n",
    "losses = []\n",
    "kl_divergences = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_kl = 0\n",
    "    for batch in data_loader:\n",
    "        x, condition = batch\n",
    "        optimizer.zero_grad()\n",
    "        recon_x, mu, log_var = cvae(x, condition)\n",
    "        \n",
    "        loss, mse, kl = cvae_loss(recon_x, x, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_kl += kl.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    avg_kl = total_kl / len(data_loader)\n",
    "    losses.append(avg_loss)\n",
    "    kl_divergences.append(avg_kl)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {avg_loss:.4f}, MSE: {mse.item():.4f}, KL: {avg_kl:.4f}\")\n",
    "\n",
    "# Generate new materials\n",
    "high_tc_values = torch.tensor([[100.0], [120.0], [150.0]], dtype=torch.float32)\n",
    "z = torch.randn((3, latent_dim))\n",
    "generated_materials = cvae.decode(z, high_tc_values).detach().numpy()\n",
    "\n",
    "# Save generated materials\n",
    "generated_df = pd.DataFrame(generated_materials, columns=features.columns)\n",
    "generated_df[\"Predicted_Tc\"] = high_tc_values.numpy()\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\optimized_materials.csv\", index=False)\n",
    "\n",
    "print(generated_df.head())\n",
    "\n",
    "# Load known superconductors dataset\n",
    "unique_m_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\unique_m.csv\")  # Replace with actual file name\n",
    "\n",
    "# Load generated superconducting materials with predicted Tc\n",
    "generated_df = pd.read_csv(r\"C:\\Users\\steve\\Downloads\\optimized_materials.csv\")  # Replace with actual file name\n",
    "\n",
    "# Standardize column names\n",
    "unique_m_df.columns = unique_m_df.columns.str.strip().str.lower()\n",
    "generated_df.columns = generated_df.columns.str.strip().str.lower()\n",
    "\n",
    "# Ensure Predicted Tc Column Exists\n",
    "predicted_tc_col = \"predicted_tc\" if \"predicted_tc\" in generated_df.columns else None\n",
    "\n",
    "if predicted_tc_col is None:\n",
    "    raise KeyError(\"Column 'Predicted_Tc' not found in generated_df\")\n",
    "predicted_tc_values = generated_df[predicted_tc_col]\n",
    "\n",
    "# Identify Common Feature Columns\n",
    "feature_columns = [col for col in generated_df.columns if col in unique_m_df.columns and col != predicted_tc_col]\n",
    "\n",
    "# Warn if features are missing in unique_m_df\n",
    "missing_cols = [col for col in feature_columns if col not in unique_m_df.columns]\n",
    "if missing_cols:\n",
    "    print(\"Warning: The following columns are missing in unique_m_df and will be ignored:\", missing_cols)\n",
    "\n",
    "# Compute Similarity Between Generated and Known Materials\n",
    "unique_features = unique_m_df[feature_columns]\n",
    "generated_features = generated_df[feature_columns]\n",
    "\n",
    "# Calculate Euclidean distance between generated materials and known materials\n",
    "distances = cdist(generated_features, unique_features, metric='euclidean')\n",
    "\n",
    "# Find closest known material for each generated one\n",
    "closest_indices = np.argmin(distances, axis=1)\n",
    "closest_materials = unique_m_df.iloc[closest_indices].reset_index(drop=True)\n",
    "\n",
    "# Append Closest Known Material Names\n",
    "if \"material\" in unique_m_df.columns:\n",
    "    generated_df[\"Closest_Material\"] = closest_materials[\"material\"]\n",
    "else:\n",
    "    raise KeyError(\"Column 'material' not found in unique_m_df. Check the dataset format.\")\n",
    "\n",
    "# Save and Display Final Results\n",
    "generated_df.to_csv(r\"C:\\Users\\steve\\Downloads\\matched_high_tc_materials.csv\", index=False)\n",
    "\n",
    "# Print a preview of matched materials\n",
    "print(generated_df[[\"Closest_Material\", \"predicted_tc\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f2d74d-b4f2-4d35-ab58-9608e0bc6b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
